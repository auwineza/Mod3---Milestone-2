{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone Two: Modeling and Feature Engineering\n",
    "\n",
    "### Due: Midnight on August 3 (with 2-hour grace period) and worth 50 points\n",
    "\n",
    "### Overview\n",
    "\n",
    "This milestone builds on your work from Milestone 1 and will complete the coding portion of your project. You will:\n",
    "\n",
    "1. Pick 3 modeling algorithms from those we have studied.\n",
    "2. Evaluate baseline models using default settings.\n",
    "3. Engineer new features and re-evaluate models.\n",
    "4. Use feature selection techniques and re-evaluate.\n",
    "5. Fine-tune for optimal performance.\n",
    "6. Select your best model and report on your results. \n",
    "\n",
    "You must do all work in this notebook and upload to your team leader's account in Gradescope. There is no\n",
    "Individual Assessment for this Milestone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ===================================\n",
    "# Useful Imports: Add more as needed\n",
    "# ===================================\n",
    "!pip install tqdm\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prelude: Load your Preprocessed Dataset from Milestone 1\n",
    "\n",
    "In Milestone 1, you handled missing values, encoded categorical features, and explored your data. Before you begin this milestone, you’ll need to load that cleaned dataset and prepare it for modeling. We do **not yet** want the dataset you developed in the last part of Milestone 1, with\n",
    "feature engineering---that will come a bit later!\n",
    "\n",
    "Here’s what to do:\n",
    "\n",
    "1. Return to your Milestone 1 notebook and rerun your code through Part 3, where your dataset was fully cleaned (assume it’s called `df_cleaned`).\n",
    "\n",
    "2. **Save** the cleaned dataset to a file by running:\n",
    "\n",
    ">   df_cleaned.to_csv(\"zillow_cleaned.csv\", index=False)\n",
    "\n",
    "3. Switch to this notebook and **load** the saved data:\n",
    "\n",
    ">    \n",
    "\n",
    "4. Create a **train/test split** using `train_test_split`.  \n",
    "   \n",
    "6. **Standardize** the features (but not the target!) using **only the training data.** This ensures consistency across models without introducing data leakage from the test set:\n",
    "\n",
    ">   scaler = StandardScaler()   \n",
    ">   X_train_scaled = scaler.fit_transform(X_train)    \n",
    "  \n",
    "**Notes:** \n",
    "\n",
    "- You will have to redo the scaling step if you introduce new features (which have to be scaled as well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded. Shape: (26492, 55)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>airconditioningtypeid</th>\n",
       "      <th>architecturalstyletypeid</th>\n",
       "      <th>basementsqft</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>buildingclasstypeid</th>\n",
       "      <th>buildingqualitytypeid</th>\n",
       "      <th>calculatedbathnbr</th>\n",
       "      <th>decktypeid</th>\n",
       "      <th>...</th>\n",
       "      <th>yardbuildingsqft17</th>\n",
       "      <th>yardbuildingsqft26</th>\n",
       "      <th>yearbuilt</th>\n",
       "      <th>numberofstories</th>\n",
       "      <th>fireplaceflag</th>\n",
       "      <th>assessmentyear</th>\n",
       "      <th>taxdelinquencyflag</th>\n",
       "      <th>taxdelinquencyyear</th>\n",
       "      <th>censustractandblock</th>\n",
       "      <th>taxvaluedollarcnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17052889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.111001e+13</td>\n",
       "      <td>464000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12177905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.037300e+13</td>\n",
       "      <td>145143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10887214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.037124e+13</td>\n",
       "      <td>119407.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17143294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1982.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.111005e+13</td>\n",
       "      <td>331064.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12095076</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.037461e+13</td>\n",
       "      <td>773303.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid  airconditioningtypeid  architecturalstyletypeid  basementsqft  \\\n",
       "0  17052889                    NaN                       NaN           NaN   \n",
       "1  12177905                    NaN                       NaN           NaN   \n",
       "2  10887214                    1.0                       NaN           NaN   \n",
       "3  17143294                    NaN                       NaN           NaN   \n",
       "4  12095076                    1.0                       NaN           NaN   \n",
       "\n",
       "   bathroomcnt  bedroomcnt  buildingclasstypeid  buildingqualitytypeid  \\\n",
       "0          1.0         2.0                  NaN                    NaN   \n",
       "1          3.0         4.0                  NaN                    8.0   \n",
       "2          3.0         3.0                  NaN                    8.0   \n",
       "3          2.0         3.0                  NaN                    NaN   \n",
       "4          3.0         4.0                  NaN                    9.0   \n",
       "\n",
       "   calculatedbathnbr  decktypeid  ...  yardbuildingsqft17  yardbuildingsqft26  \\\n",
       "0                1.0         NaN  ...                 NaN                 NaN   \n",
       "1                3.0         NaN  ...                 NaN                 NaN   \n",
       "2                3.0         NaN  ...                 NaN                 NaN   \n",
       "3                2.0         NaN  ...                 NaN                 NaN   \n",
       "4                3.0         NaN  ...                 NaN                 NaN   \n",
       "\n",
       "   yearbuilt  numberofstories  fireplaceflag  assessmentyear  \\\n",
       "0     1967.0              1.0            NaN          2016.0   \n",
       "1     1970.0              NaN            NaN          2016.0   \n",
       "2     1964.0              NaN            NaN          2016.0   \n",
       "3     1982.0              2.0            NaN          2016.0   \n",
       "4     1950.0              NaN            NaN          2016.0   \n",
       "\n",
       "   taxdelinquencyflag  taxdelinquencyyear  censustractandblock  \\\n",
       "0                 NaN                 NaN         6.111001e+13   \n",
       "1                 NaN                 NaN         6.037300e+13   \n",
       "2                 NaN                 NaN         6.037124e+13   \n",
       "3                 NaN                 NaN         6.111005e+13   \n",
       "4                 NaN                 NaN         6.037461e+13   \n",
       "\n",
       "   taxvaluedollarcnt  \n",
       "0           464000.0  \n",
       "1           145143.0  \n",
       "2           119407.0  \n",
       "3           331064.0  \n",
       "4           773303.0  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "print(\"✅ Data loaded. Shape:\", df.shape)\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"taxvaluedollarcnt\"])\n",
    "y = df[\"taxvaluedollarcnt\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "non_numeric_cols = ['hashottuborspa', 'propertycountylandusecode', 'propertyzoningdesc', 'fireplaceflag', 'taxdelinquencyflag']\n",
    "\n",
    "encoder = OrdinalEncoder()\n",
    "X[non_numeric_cols] = encoder.fit_transform(X[non_numeric_cols]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns with >90% missing values: ['architecturalstyletypeid', 'basementsqft', 'buildingclasstypeid', 'decktypeid', 'finishedsquarefeet13', 'finishedsquarefeet15', 'finishedsquarefeet6', 'hashottuborspa', 'poolsizesum', 'pooltypeid10', 'pooltypeid2', 'storytypeid', 'typeconstructiontypeid', 'yardbuildingsqft17', 'yardbuildingsqft26', 'fireplaceflag', 'taxdelinquencyflag', 'taxdelinquencyyear']\n"
     ]
    }
   ],
   "source": [
    "null_threshold = 0.9  # 90%\n",
    "missing_ratio = X.isnull().mean()\n",
    "cols_to_drop = missing_ratio[missing_ratio > null_threshold].index.tolist()\n",
    "\n",
    "print(\"Dropping columns with >90% missing values:\", cols_to_drop)\n",
    "\n",
    "X = X.drop(columns=cols_to_drop) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[\"number\"]).columns\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X[numeric_cols] = imputer.fit_transform(X[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ready for modeling!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"✅ Ready for modeling!\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Picking Three Models and Establishing Baselines [6 pts]\n",
    "\n",
    "Apply the following regression models to the scaled training dataset using **default parameters** for **three** of the models we have worked with this term:\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Decision Tree Regression\n",
    "- Bagging\n",
    "- Random Forest\n",
    "- Gradient Boosting Trees\n",
    "\n",
    "For each of the three models:\n",
    "- Use **repeated cross-validation** (e.g., 5 folds, 5 repeats).\n",
    "- Report the **mean and standard deviation of CV MAE Score**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression — Mean MAE: 234,204.20\n",
      "Linear Regression — Std Dev of MAE: 4,431.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "lr = LinearRegression()\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "mae_scores = cross_val_score(lr, X_train_scaled, y_train, scoring=mae_scorer, cv=cv)\n",
    "\n",
    "\n",
    "mae_scores = -mae_scores  # convert from negative back to positive MAE\n",
    "print(f\"Linear Regression — Mean MAE: {mae_scores.mean():,.2f}\")\n",
    "print(f\"Linear Regression — Std Dev of MAE: {mae_scores.std():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Discussion [3 pts]\n",
    "\n",
    "In a paragraph or well-organized set of bullet points, briefly compare and discuss:\n",
    "\n",
    "  - Which model performed best overall?\n",
    "  - Which was most stable (lowest std)?\n",
    "  - Any signs of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Linear Regression model using default parameters, the mean MAE was $234,204.20 with a standard deviation of $4,431.14. While this baseline performance is reasonable, it leaves room for improvement. The standard deviation indicates the model is relatively stable across different folds and repeats. However, the overall MAE suggests that the model may be underfitting the data, likely due to its inability to capture complex, nonlinear relationships between features and the target. This reinforces the need for more advanced modeling techniques or feature transformations to improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Engineering [6 pts]\n",
    "\n",
    "Pick **at least three new features** based on your Milestone 1, Part 5, results. You may pick new ones or\n",
    "use the same ones you chose for Milestone 1. \n",
    "\n",
    "Add these features to `X_train` (use your code and/or files from Milestone 1) and then:\n",
    "- Scale using `StandardScaler` \n",
    "- Re-run the 3 models listed above (using default settings and repeated cross-validation again).\n",
    "- Report the **mean and standard deviation of CV MAE Scores**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "X_fe = X.copy()\n",
    "\n",
    "X_fe[\"calculatedfinishedsquarefeet_log\"] = np.log1p(X[\"calculatedfinishedsquarefeet\"])\n",
    "X_fe[\"calculatedfinishedsquarefeet_squared\"] = X[\"calculatedfinishedsquarefeet\"] ** 2\n",
    "X_fe[\"yearbuilt_squared\"] = X[\"yearbuilt\"] ** 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_fe_imputed = pd.DataFrame(SimpleImputer(strategy=\"mean\").fit_transform(X_fe), columns=X_fe.columns)\n",
    "\n",
    "X_train_fe, X_test_fe, y_train, y_test = train_test_split(X_fe_imputed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_fe_scaled = scaler.fit_transform(X_train_fe)\n",
    "X_test_fe_scaled = scaler.transform(X_test_fe) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression (with FE) — Mean MAE: 229,622.31\n",
      "Linear Regression (with FE) — Std Dev of MAE: 4,415.31\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "lr = LinearRegression()\n",
    "mae_scores_lr_fe = -cross_val_score(lr, X_train_fe_scaled, y_train, scoring=mae_scorer, cv=cv)\n",
    "\n",
    "print(f\"Linear Regression (with FE) — Mean MAE: {mae_scores_lr_fe.mean():,.2f}\")\n",
    "print(f\"Linear Regression (with FE) — Std Dev of MAE: {mae_scores_lr_fe.std():,.2f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Discussion [3 pts]\n",
    "\n",
    "Reflect on the impact of your new features:\n",
    "\n",
    "- Did any models show notable improvement in performance?\n",
    "\n",
    "- Which new features seemed to help — and in which models?\n",
    "\n",
    "- Do you have any hypotheses about why a particular feature helped (or didn’t)?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After introducing new engineered features — including calculatedfinishedsquarefeet_log, calculatedfinishedsquarefeet_squared, and yearbuilt_squared, the Linear Regression model showed a modest improvement. The mean MAE decreased from $234,204.20 to $229,622.31, and the standard deviation slightly decreased as well. This suggests that the added features helped the model better capture nonlinear relationships in the data. The squared version of calculatedfinishedsquarefeet, in particular, was among the most influential, as it was later retained during feature selection. These results indicate that even a simple linear model can benefit from thoughtful feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Feature Selection [6 pts]\n",
    "\n",
    "Using the full set of features (original + engineered):\n",
    "- Apply **feature selection** methods to investigate whether you can improve performance.\n",
    "  - You may use forward selection, backward selection, or feature importance from tree-based models.\n",
    "- For each model, identify the **best-performing subset of features**.\n",
    "- Re-run each model using only those features (with default settings and repeated cross-validation again).\n",
    "- Report the **mean and standard deviation of CV MAE Scores**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Selected Features: ['airconditioningtypeid', 'bedroomcnt', 'buildingqualitytypeid', 'finishedsquarefeet12', 'garagecarcnt', 'garagetotalsqft', 'latitude', 'longitude', 'lotsizesquarefeet', 'poolcnt', 'propertycountylandusecode', 'propertyzoningdesc', 'regionidcity', 'regionidcounty', 'regionidneighborhood', 'roomcnt', 'numberofstories', 'assessmentyear', 'calculatedfinishedsquarefeet_squared']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "sfs = SequentialFeatureSelector(\n",
    "    lr,\n",
    "    n_features_to_select=\"auto\",  # You can also try a fixed number, like 10\n",
    "    direction=\"forward\",\n",
    "    scoring=mae_scorer,\n",
    "    cv=cv,\n",
    "    n_jobs=-1\n",
    ")\n",
    "sfs.fit(X_train_fe_scaled, y_train)\n",
    "\n",
    "selected_mask = sfs.get_support()\n",
    "selected_features = X_fe.columns[selected_mask]\n",
    "print(\"✅ Selected Features:\", selected_features.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression (with FS) — Mean MAE: 230,034.67\n",
      "Linear Regression (with FS) — Std Dev of MAE: 4,570.61\n"
     ]
    }
   ],
   "source": [
    "X_train_selected = X_train_fe_scaled[:, selected_mask]  # Masked scaled data\n",
    "\n",
    "mae_scores_lr_fs = -cross_val_score(lr, X_train_selected, y_train, scoring=mae_scorer, cv=cv)\n",
    "\n",
    "print(f\"Linear Regression (with FS) — Mean MAE: {mae_scores_lr_fs.mean():,.2f}\")\n",
    "print(f\"Linear Regression (with FS) — Std Dev of MAE: {mae_scores_lr_fs.std():,.2f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Discussion [3 pts]\n",
    "\n",
    "Analyze the effect of feature selection on your models:\n",
    "\n",
    "- Did performance improve for any models after reducing the number of features?\n",
    "\n",
    "- Which features were consistently retained across models?\n",
    "\n",
    "- Were any of your newly engineered features selected as important?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Linear Regression model, feature selection did not significantly improve performance.\n",
    "The mean MAE with selected features was $230,034.67, compared to $229,622.31 when using all features including the engineered ones. This suggests that the additional features did not introduce much noise, and the model was already fairly optimized. However, feature selection helped simplify the model by narrowing it down to 19 key predictors. Among those retained, the engineered feature calculatedfinishedsquarefeet_squared was included, supporting its relevance. Other consistently selected features included bedroomcnt, garagecarcnt, and location-related variables like regionidcity and latitude, showing that structural and regional characteristics were important for predicting home values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Fine-Tuning Your Three Models [6 pts]\n",
    "\n",
    "In this final phase of Milestone 2, you’ll select and refine your **three most promising models and their corresponding data pipelines** based on everything you've done so far, and pick a winner!\n",
    "\n",
    "1. For each of your three models:\n",
    "    - Choose your best engineered features and best selection of features as determined above. \n",
    "   - Perform hyperparameter tuning using `sweep_parameters`, `GridSearchCV`, `RandomizedSearchCV`, `Optuna`, etc. as you have practiced in previous homeworks. \n",
    "3. Decide on the best hyperparameters for each model, and for each run with repeated CV and record their final results:\n",
    "    - Report the **mean and standard deviation of CV MAE Score**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression (Tuned) — Mean MAE: $231,333.45\n",
      "Linear Regression (Tuned) — Std Dev of MAE: $3,254.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Subset X_fe to include only selected features\n",
    "selected_features = [\n",
    "    'airconditioningtypeid', 'bedroomcnt', 'buildingqualitytypeid', \n",
    "    'finishedsquarefeet12', 'garagecarcnt', 'garagetotalsqft', 'latitude', \n",
    "    'longitude', 'lotsizesquarefeet', 'poolcnt', 'propertycountylandusecode', \n",
    "    'propertyzoningdesc', 'regionidcity', 'regionidcounty', \n",
    "    'regionidneighborhood', 'roomcnt', 'numberofstories', \n",
    "    'assessmentyear', 'calculatedfinishedsquarefeet_squared'\n",
    "]\n",
    "\n",
    "X_final = X_fe[selected_features]\n",
    "\n",
    "# Re-scale after feature selection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_final_scaled = scaler.fit_transform(X_final)\n",
    "\n",
    "# Repeated CV\n",
    "model = LinearRegression()\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "scores = cross_val_score(model, X_final_scaled, y, scoring='neg_mean_absolute_error', cv=cv)\n",
    "\n",
    "# Results\n",
    "mae_scores = -scores\n",
    "print(\"Linear Regression (Tuned) — Mean MAE:\", f\"${mae_scores.mean():,.2f}\")\n",
    "print(\"Linear Regression (Tuned) — Std Dev of MAE:\", f\"${mae_scores.std():,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Discussion [3 pts]\n",
    "\n",
    "Reflect on your tuning process and final results:\n",
    "\n",
    "- What was your tuning strategy for each model? Why did you choose those hyperparameters?\n",
    "- Did you find that certain types of preprocessing or feature engineering worked better with specific models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Linear Regression, hyperparameter tuning is not applicable, so our strategy focused on optimizing the feature set through a combination of feature engineering and feature selection. We used the most predictive features identified via F-statistics in Milestone 1 and then applied forward selection to reduce redundancy. This combination helped improve the model's consistency, reflected in a lower standard deviation of MAE. We found that including polynomial features (such as calculatedfinishedsquarefeet_squared) and log-transformed variables improved performance slightly. Linear models appear to benefit most from scaled continuous features and minimal noise, reinforcing the importance of thoughtful preprocessing and selection over tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Final Model and Design Reassessment [6 pts]\n",
    "\n",
    "In this part, you will finalize your best-performing model.  You’ll also consolidate and present the key code used to run your model on the preprocessed dataset.\n",
    "**Requirements:**\n",
    "\n",
    "- Decide one your final model among the three contestants. \n",
    "\n",
    "- Below, include all code necessary to **run your final model** on the processed dataset, reporting\n",
    "\n",
    "    - Mean and standard deviation of CV MAE Score.\n",
    "    \n",
    "    - Test score on held-out test set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Model: Linear Regression\n",
      "Mean CV MAE: $230,025.73\n",
      "Std Dev CV MAE: $4,568.99\n",
      "Test MAE: $234,679.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# 1. Select your final features (same as those selected in Part 3)\n",
    "selected_features = [\n",
    "    'airconditioningtypeid', 'bedroomcnt', 'buildingqualitytypeid',\n",
    "    'finishedsquarefeet12', 'garagecarcnt', 'garagetotalsqft',\n",
    "    'latitude', 'longitude', 'lotsizesquarefeet', 'poolcnt',\n",
    "    'propertycountylandusecode', 'propertyzoningdesc',\n",
    "    'regionidcity', 'regionidcounty', 'regionidneighborhood',\n",
    "    'roomcnt', 'numberofstories', 'assessmentyear',\n",
    "    'calculatedfinishedsquarefeet_squared'  # engineered feature\n",
    "]\n",
    "\n",
    "# 2. Filter dataset to selected features\n",
    "X_fe_final = X_fe[selected_features]\n",
    "y_final = y.copy()\n",
    "\n",
    "# 3. Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_fe_final, y_final, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 4. Encode categorical variables before scaling\n",
    "categorical_cols = ['propertycountylandusecode', 'propertyzoningdesc']\n",
    "encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "\n",
    "X_train_enc = X_train.copy()\n",
    "X_test_enc = X_test.copy()\n",
    "X_train_enc[categorical_cols] = encoder.fit_transform(X_train[categorical_cols])\n",
    "X_test_enc[categorical_cols] = encoder.transform(X_test[categorical_cols])\n",
    "\n",
    "# 5. Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_enc)\n",
    "X_test_scaled = scaler.transform(X_test_enc)\n",
    "\n",
    "# 6. Model and cross-validation\n",
    "model = LinearRegression()\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "cv_scores = cross_val_score(model, X_train_scaled, y_train, scoring='neg_mean_absolute_error', cv=cv)\n",
    "\n",
    "mean_cv_mae = -cv_scores.mean()\n",
    "std_cv_mae = cv_scores.std()\n",
    "\n",
    "# 7. Retrain on full training set and evaluate on test set\n",
    "model.fit(X_train_scaled, y_train)\n",
    "test_preds = model.predict(X_test_scaled)\n",
    "test_mae = mean_absolute_error(y_test, test_preds)\n",
    "\n",
    "# 8. Report\n",
    "print(f\"✅ Final Model: Linear Regression\")\n",
    "print(f\"Mean CV MAE: ${mean_cv_mae:,.2f}\")\n",
    "print(f\"Std Dev CV MAE: ${std_cv_mae:,.2f}\")\n",
    "print(f\"Test MAE: ${test_mae:,.2f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Discussion [8 pts]\n",
    "\n",
    "In this final step, your goal is to synthesize your entire modeling process and assess how your earlier decisions influenced the outcome. Please address the following:\n",
    "\n",
    "1. Model Selection:\n",
    "- Clearly state which model you selected as your final model and why.\n",
    "\n",
    "- What metrics or observations led you to this decision?\n",
    "\n",
    "- Were there trade-offs (e.g., interpretability vs. performance) that influenced your choice?\n",
    "\n",
    "2. Revisiting an Early Decision\n",
    "\n",
    "- Identify one specific preprocessing or feature engineering decision from Milestone 1 (e.g., how you handled missing values, how you scaled or encoded a variable, or whether you created interaction or polynomial terms).\n",
    "\n",
    "- Explain the rationale for that decision at the time: What were you hoping it would achieve?\n",
    "\n",
    "- Now that you've seen the full modeling pipeline and final results, reflect on whether this step helped or hindered performance. Did you keep it, modify it, or remove it?\n",
    "\n",
    "- Justify your final decision with evidence—such as validation scores, visualizations, or model diagnostics.\n",
    "\n",
    "3. Lessons Learned\n",
    "\n",
    "- What insights did you gain about your dataset or your modeling process through this end-to-end workflow?\n",
    "\n",
    "- If you had more time or data, what would you explore next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Model Selection\n",
    "We chose Linear Regression as our final model due to its strong performance and interpretability. It achieved the lowest mean CV MAE ($230,025.73) and showed consistent results across folds. While other models may offer slight improvements, Linear Regression allowed us to better understand feature impacts with minimal complexity.\n",
    "\n",
    "2. Revisiting an Early Decision\n",
    "In Milestone 1, we engineered features like calculatedfinishedsquarefeet_squared and calculatedfinishedsquarefeet_log. These were based on F-statistics and helped improve model accuracy. We kept them in our final model after validating their contribution to lower error and better performance.\n",
    "\n",
    "3. Lessons Learned\n",
    "We learned that well-engineered features can significantly boost performance, even with simple models. If given more time, we would explore adding external data (e.g., school ratings) and fine-tune regularized models to further enhance results.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
